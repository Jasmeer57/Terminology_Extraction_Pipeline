# ğŸ§¬ Terminology Extraction Pipeline Tool (PubMed Pipeline Tool)

The **Terminology Extraction Pipeline Tool** is a comprehensive, user-friendly application designed to streamline biomedical research workflows by automating the process of querying [PubMed](https://pubmed.ncbi.nlm.nih.gov/), extracting abstracts, performing terminology extraction, entity linking to biomedical ontologies, and analyzing term embeddings using state-of-the-art models â€” all accessible via an intuitive graphical user interface (GUI). The tool also supports Docker for easy deployment across environments.

---

## âœ¨ Features

* ğŸ” **PubMed Search**
  Query PubMed with user-provided search strings and retrieve relevant abstracts.

* ğŸ“ **Abstract Extraction**
  Automatically fetch abstracts from PubMed articles for downstream analysis.

* ğŸ§  **Term Extraction**
  Extract biomedical terms from abstracts to identify key concepts.

* ğŸ”— **Entity Linking**
  Link extracted terms to ontologies using the [Ontology Lookup Service (OLS) API](https://www.ebi.ac.uk/ols/index) to provide semantic context.

* ğŸ¤– **Model Comparison**
  Generate and compare embeddings of extracted terms using multiple HuggingFace transformer models such as `BioBERT`, `SciBERT`, and others.

* ğŸ“Š **Clustering**
  Group terms via the DBSCAN clustering algorithm to identify meaningful clusters and relationships.

* ğŸ–¥ï¸ **Intuitive GUI**
  A simple graphical interface allows easy input of search queries and user credentials, streamlining interaction.

* ğŸ³ **Docker Support**
  Includes pre-configured Dockerfile and `docker-compose.yml` for hassle-free containerized deployment.

* âš™ï¸ **Customizable Configuration**
  Configure parameters like search term, output directory, and model choices through a `config.json` file.
  
* ğŸ› ï¸ **Snakemake Workflow**
  Reproducible, modular, and scalable pipeline with dynamic input via command line.

---

## ğŸš€ Getting Started

### Download Guide
#### 1. Clone the Repository

```bash
git clone https://github.com/your-username/pubmed-pipeline-tool.git
cd pubmed-pipeline-tool
```

### ğŸ”§ Option 1: Local Setup (with Conda)

```bash
conda env create -f environment.yml
conda activate pubmed-env
```

Run the pipeline:

```bash
snakemake --cores 4 --config email="your@email.com" search_term="cancer biomarkers"
```

### ğŸ³ Option 2: Docker Setup

```bash
docker-compose up --build
```

---

## ğŸ—‚ï¸ Project Structure

```
terminology-pipeline/
â”œâ”€â”€ Snakefile
â”œâ”€â”€ config.yaml
â”œâ”€â”€ rules/
â”‚   â”œâ”€â”€ fetch.smk
â”‚   â”œâ”€â”€ extract.smk
â”‚   â”œâ”€â”€ link.smk
â”‚   â”œâ”€â”€ embed.smk
â”‚   â””â”€â”€ cluster.smk
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ *.py
â”œâ”€â”€ environment.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ results/
```

---

## âš™ï¸ Configuration

While `email` and `search_term` are provided via the CLI, defaults and model settings live in `config.yaml`:

```yaml
output_dir: "results/"
huggingface_models:
  - dmis-lab/biobert-base-cased-v1.1
  - allenai/scibert_scivocab_uncased
```

---

## ğŸ“„ Output Files

All outputs are stored in the `results/` directory:

* `abstracts.json` â€“ Raw PubMed abstracts
* `extracted_terms.json` â€“ Biomedical terms extracted
* `linked_entities.json` â€“ Ontology mappings via OLS
* `embeddings_comparison.csv` â€“ Embedding similarity scores
* `clusters.json` â€“ Grouped terms from DBSCAN

---

## ğŸ“¦ Dependencies

Dependencies are handled via Conda. Defined in `environment.yml`:

```yaml
- biopython
- pandas
- tqdm
- scikit-learn
- requests
- snakemake
- sentence-transformers (via pip)
```


### Option 3. Install Python Dependencies

```bash
pip install biopython pandas tqdm transformers sentence-transformers scikit-learn requests tkinter
```

Alternatively, all dependencies are listed in `requirements.txt` and will be installed automatically when using Docker.

### 3. Configure

Edit the `config.json` file to set your email, search term, output directory, and preferred HuggingFace models:

```json
{
  "email": "your-email@example.com",
  "search_term": "cancer genomics",
  "output_dir": "./results",
  "huggingface_models": [
    "dmis-lab/biobert-base-cased-v1.1",
    "allenai/scibert_scivocab_uncased"
  ]
}
```
The GUI will be accessible at `http://localhost:8501`.


## ğŸ§ª Example Use Case

1. Launch the GUI (locally or via Docker).
2. Enter your PubMed search query (e.g., *"Alzheimer's disease biomarkers"*).
3. Input your email for NCBI compliance.
4. Click **Run** and wait for the processing to complete.
5. Explore and utilize the structured output data for research, machine learning, or visualization tasks.


---

## ğŸ™‹â€â™€ï¸ Contributing

Contributions, issues, and feature requests are welcome! Feel free to open an issue or submit a pull request.

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ“¬ Contact

For questions or collaboration, please contact:

**Jasmeer Singh Kalra**
ğŸ“§ [jasmeer57@gmail.com](mailto:jasmeer57@gmail.com)
ğŸ”— [LinkedIn](https://www.linkedin.com/in/jasmeer-singh/)
ğŸ™ [GitHub](https://github.com/Jasmeer57)

---

## â­ Acknowledgments

* HuggingFace for state-of-the-art biomedical transformer models
* NCBI and PubMed for access to invaluable biomedical research data
* EBI Ontology Lookup Service (OLS) for ontology linking capabilities

---
