# ğŸ§¬ Terminology Extraction Pipeline

The **Terminology Extraction Pipeline** is a modular and reproducible workflow designed for mining biomedical literature from [PubMed](https://pubmed.ncbi.nlm.nih.gov/). Powered by **Snakemake**, it automates the entire processâ€”from abstract retrieval and term extraction to ontology linking, embedding comparison, and clusteringâ€”making it ideal for research and analysis at scale.

---

## âœ¨ Features

* ğŸ” **PubMed Search**: Dynamically fetch abstracts using a user-defined query and email (NCBI-compliant).
* ğŸ§  **Terminology Extraction**: Identify biomedical terms from abstracts.
* ğŸ”— **Ontology Linking**: Map terms to ontologies using the [OLS API](https://www.ebi.ac.uk/ols/index).
* ğŸ¤– **Embedding Comparison**: Compare semantic similarity using HuggingFace models like **BioBERT**, **SciBERT**, etc.
* ğŸ“Š **Clustering**: Group related terms using **DBSCAN**.
* ğŸ› ï¸ **Snakemake Workflow**: Reproducible, modular, and scalable pipeline with dynamic input via command line.
* ğŸ³ **Docker Support**: Easily deploy the pipeline in a containerized environment.

---

## ğŸš€ Getting Started

### ğŸ”§ Option 1: Local Setup (with Conda)

```bash
conda env create -f environment.yml
conda activate pubmed-env
```

Run the pipeline:

```bash
snakemake --cores 4 --config email="your@email.com" search_term="cancer biomarkers"
```

### ğŸ³ Option 2: Docker Setup

```bash
docker-compose up --build
```

---

## ğŸ—‚ï¸ Project Structure

```
terminology-pipeline/
â”œâ”€â”€ Snakefile
â”œâ”€â”€ config.yaml
â”œâ”€â”€ rules/
â”‚   â”œâ”€â”€ fetch.smk
â”‚   â”œâ”€â”€ extract.smk
â”‚   â”œâ”€â”€ link.smk
â”‚   â”œâ”€â”€ embed.smk
â”‚   â””â”€â”€ cluster.smk
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ *.py
â”œâ”€â”€ environment.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ results/
```

---

## âš™ï¸ Configuration

While `email` and `search_term` are provided via the CLI, defaults and model settings live in `config.yaml`:

```yaml
output_dir: "results/"
huggingface_models:
  - dmis-lab/biobert-base-cased-v1.1
  - allenai/scibert_scivocab_uncased
```

---

## ğŸ“„ Output Files

All outputs are stored in the `results/` directory:

* `abstracts.json` â€“ Raw PubMed abstracts
* `extracted_terms.json` â€“ Biomedical terms extracted
* `linked_entities.json` â€“ Ontology mappings via OLS
* `embeddings_comparison.csv` â€“ Embedding similarity scores
* `clusters.json` â€“ Grouped terms from DBSCAN

---

## ğŸ“¦ Dependencies

Dependencies are handled via Conda. Defined in `environment.yml`:

```yaml
- biopython
- pandas
- tqdm
- scikit-learn
- requests
- snakemake
- sentence-transformers (via pip)
```

---

## ğŸ™‹ Contributing

Open to pull requests and discussions. Feel free to contribute enhancements or report issues!

---

## ğŸ“„ License

MIT License. See [`LICENSE`](LICENSE) for more information.

---


